[
["index.html", "Quantitative Research Methods for Education 1 Introduction 1.1 Intro to Rmarkdown 1.2 Intro to Bookdown 1.3 Resources to Rmarkdown 1.4 Resources to Bookdown", " Quantitative Research Methods for Education Wenliang He 2018-07-05 1 Introduction Let’s test it a little bit! 1.1 Intro to Rmarkdown Add some text and see what happens. 1.2 Intro to Bookdown 1.3 Resources to Rmarkdown 1.4 Resources to Bookdown "],
["a-brief-r-tutorial.html", "2 A Brief R Tutorial 2.1 Essential Basics - intro 2.2 Atomic Data Types - atomic-dtype", " 2 A Brief R Tutorial 2.1 Essential Basics - intro In R, every method is a function: name + (). For example, in your R console, try type help(). 2.2 Atomic Data Types - atomic-dtype There are four commonly used atomic data types. logical: e.g. TRUE, FALSE integer: e.g. -2, 0, 1, 2 numeric: e.g., -2.0, 0.05, 1.2 character: e.g., “-2.0”, “cat”, “dog” 2.2.1 Logical - logical Logical is simply TRUE or FALSE. To note, T is a shorthand for TRUE. They are strictly identical. TRUE ## [1] TRUE T ## [1] TRUE To check the data type of TRUE, use the class() function. class(TRUE) ## [1] &quot;logical&quot; class(T) ## [1] &quot;logical&quot; To note, you MUST NOT use True or False, which would result in an error. True ## Error in eval(expr, envir, enclos): object &#39;True&#39; not found 2.2.2 Integer - integer If you directly type 2 into the R console, you might think it’s an integer, but it is in fact of numeric data type. To note, numeric data type is also called floating point （浮点型数据） in other programming languages. 2; class(2) ## [1] 2 ## [1] &quot;numeric&quot; The ; sign is used to separate commands such that two lines can be written succinctly（简洁的） in one line. In our case, 2; class(2) is identical to 2 class(2) To have an integer, you can attach an L to the integer. 2L; class(2L) ## [1] 2 ## [1] &quot;integer&quot; getwd() ## [1] &quot;/Users/Wen/百度云同步盘/01_Teaching/01_My_Courses/11_QuantMethods_I&quot; xdat = read.csv(&quot;Data/demo_data.csv&quot;, stringsAsFactors = F) xvec = xdat[,&quot;session&quot;] # extract the feature &#39;session&#39; xvec = xdat$session # identical cnts = table(xvec) cnts ## xvec ## SS09 SS10 SS11 ## 58 51 91 pcts = 100 * table(xvec) / sum( table(xvec) ) pcts ## xvec ## SS09 SS10 SS11 ## 29.0 25.5 45.5 mat = cbind(cnts, pcts) mat ## cnts pcts ## SS09 58 29.0 ## SS10 51 25.5 ## SS11 91 45.5 "],
["linear-regression.html", "3 Linear Regression 3.1 Table of Contents 3.2 Pearson Product-Moment Correlation Coefficient 3.3 Simple Linear Regression 3.4 Non-linear Relations 3.5 The Assumptions of Simple Linear Regression 3.6 Assumption Diagnostics", " 3 Linear Regression 3.1 Table of Contents Pearson Correlation Simple Linear Regression Scatterplots Transformations 3.2 Pearson Product-Moment Correlation Coefficient 3.2.1 Definition Commonly referred to as Pearson correlation or simply correlation, Pearson product-moment correlation coefficient is a standardized form of the covariance. Given the formula for variance, \\[ Var(Y) = \\frac{\\sum_{i=1}^n(Y_i - \\bar{Y})^2}{n-1} \\] one should not be too surprised to see the formula for covariance. \\[ Cov(X,Y) = \\frac{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})}{n-1} \\] It should thus be clear that variance of Y is the covariance of Y with Y itself, i.e., variance is a special case of covariance. \\[ Cov(Y,Y) = \\frac{\\sum_{i=1}^n(Y_i - \\bar{Y})(Y_i-\\bar{Y})}{n-1} = \\frac{\\sum_{i=1}^n(Y_i - \\bar{Y})^2}{n-1} = Var(Y) \\] If we divide \\(Cov(X,Y)\\) by the corresponding standard deviations involved with regard to X and Y, we get Pearson correlation. In other words, Pearson correlation is the covariance of z-scores. \\[ r = \\frac {\\color{red}{Cov(X,Y)}} {s_Xs_Y} = \\frac {\\color{red}{\\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})}} {s_X s_Y \\color{red}{(n-1)}} = \\frac{1}{n-1} \\sum_{i=1}^n \\color{gold}{\\frac{(X_i-\\bar{X})}{s_X}} \\color{blue}{\\frac{(Y_i-\\bar{Y})}{s_Y}} = \\frac{1}{n-1} \\sum_{i=1}^n \\color{gold}{z_X} \\color{blue}{z_Y} \\] 3.2.2 Properties of Pearson Correlation The correlation \\(r\\) is always between -1 and 1. Values of \\(r\\) close to 0 indicate a weak linear relationship. Values of \\(r\\) close to 1 imply a strong positive linear relationship, whereas values close to -1 imply a negative linear relationship. The extreme cases of \\(r=-1\\) or \\(r=1\\) occur only when the points in a scatterplot lie exactly along a straight line. In other words, signs of \\(r\\) show the direction and absolute values of \\(r\\) indicate the strength of the relationship. The correlation \\(r\\) does not change when we change the units of measurement of either X or Y or both. This should be clear since correlation \\(r\\) uses the unitless z-scores. The correlation \\(r\\) does not distinguish between independent and dependent variables. Reversing X and Y gives identical results. It has to be emphasized again that statistics knows no causal directions. Statistics on its own only reveals associations, not causations. 3.2.3 Limitations of Pearson Correlation Pearson correlation measures only linear association. Always plot your data before calculating \\(r\\). TODO: need a graph here! Calculation of Pearson correlation invovles means and standard deviations, and hence \\(r\\) is susceptible to outliers. Always plot your data and look for potentially influential data points (which is defined later). 3.2.4 Other Issues on the Usage of Pearson Correlation Pearson correlation based on averaged data is usually higher than the correlation between the same variables using data for individuals. For example, the average weight of infants against their age in months would give a very strong positive correlation near one. A plot of weight against age for individual infants will show much more scatter and lower correlation. On the contrary, using a composite score averaged from multiple items measuring motivation usually gives much more consistent, and hence reliable, result. When the data we use do not contain information on the full range of independent and dependent variables, we have the restricted-range problem. When data suffer from restricted range, \\(r\\) is usually lower than it would be if the full range could be observed. TODO: need an example here! Lurking variables can make correlation results misleading. This problem is not specific to Pearson correlation. All measures of two-way relationships suffer from this problem, which is known as the interaction effect. TODO: need a graph here! Extrapolation (using a model far beyond the range of data used to fit it) often produces unreliable predictions. This problem is not specific to Pearson correlation. All models suffer from some generalizability concern, which might raise the concern of omitted-variable bias (e.g., individuals possessing values beyond the observed range of values might systematically differ from individuals currently in the sample). 3.3 Simple Linear Regression 3.3.1 Independent Variable: From Categorical to Continuous mid1cat = cut(xdat$mid1, breaks=quantile(xdat$mid1), include.lowest=T) xdat = data.frame(xdat, mid1cat) boxplot(mid3 ~ mid1cat, data=xdat, main=&quot;Boxplot of Exam Results&quot;, xlab=&quot;First Midterm Score Category&quot;, ylab=&quot;Final Exam Score&quot;) abline(h=mean(xdat$mid3,na.rm=T), lty=2) plot(mid3 ~ mid1, data=xdat, main=&quot;Scatterplot of Exam Results&quot;, xlab=&quot;First Midterm Score&quot;, ylab=&quot;Final Exam Score&quot;) mod = lm(mid3 ~ mid1, data=xdat) abline(h=mean(xdat$mid3,na.rm=T), lty=2) abline(coef(mod)[1], coef(mod)[2], col=&quot;red&quot;) 3.3.2 Fitting a Straight Line to Data Let us fit a straight line with an intercept and a slope. As a result, the conditional expectation of Y is now a function of X and the prediction model with unknown population parameters can be expressed as follows. To note, “the conditional expectation of Y” is just a fancy way of saying “the predicted values of Y” or “the prediction model of Y”. They are all different ways of saying the same thing. \\[ E(Y|X) = \\beta_0 + \\beta_1X \\] Accordingly, the predition model of Y with sample parameters is: \\[ \\hat{E}(Y_i|X_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i \\] In practice, we often use simplified notations to represent the preceding formula. \\[ \\hat{Y}_i = b_0 + b_1X_i \\] where \\(\\hat{Y}_i \\equiv \\hat{E}(Y_i|X_i)\\), \\(b_0 \\equiv \\hat{\\beta}_0\\) and \\(b_1 \\equiv \\hat{\\beta}_1\\). This is the formula for the deterministic part of the linear regression. By adding the stochastic part, we have \\[ Y_i = \\hat{Y}_i + \\hat{R}_i = b_0 + b_1X_i + e_i \\] We can describe the previous simple linear model as the regression of Y on X. At this moment, this expression might be exceedingly clear to you, since the use of Y and X makes it abundently clear which one is the DV and which one is the IV. In a real case, however, one might be confused over which is which. For example, with the expression “regressing income on education”, one must know that income is Y and education is X. 3.3.3 The Least-Squares Method The least-squares method is one of the ways to solve the above equation for the unknown parameters \\(b_0\\) and \\(b_1\\). The official name for this method is ordinary least squares (OLS) method. \\[ SSR = \\sum_{i=1}^n R_i^2 = \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2 \\] The idea is that we would find \\(b_0\\) and \\(b_1\\) such that \\(SSR\\) would reach its minimum. Remember, the last time we use this idea, we end up finding the mean. \\[ SSR = \\sum_{i=1}^n (Y_i-c)^2 \\] Solving for \\(c\\) that minimizes \\(SSR\\) gives \\(c = \\bar{Y}\\). Applying the idea above, we would eventually find the following solutions. \\[ b_1 = \\frac{\\sum(X_i-\\bar{X})(Y_i-\\bar{Y})} {\\sum(X_i-\\bar{X})^2} \\] and \\[ b_0 = \\bar{Y} - b_1 \\bar{X} \\] 3.3.4 Interpretations of Regression Coefficients It should be clear that \\(b_0\\) is the intercept, which is interpreted as the value of Y when X = 0. To note, values of X oftentimes cannot be exactly 0, e.g., weight or IQ. In such a case, the interpretation of \\(b_0\\) would not make intuitive sense (since no one would have 0 weight or 0 intelligence); the intercept is there only to make the math work. To make the interpretation of \\(b_0\\) meaningful, we can choose to center X with respect to its mean, i.e. \\(X^* = X - \\bar{X}\\). Now \\(b_0\\) is the value of Y when \\(X = \\bar{X}\\). In contrast, \\(b_1\\) is the slope, which is interpreted as the amount of change in Y when X changes by 1 unit of measurement. As a result, the value of \\(b_1\\) would change, if the unit of measurement for Y or X or both changes. 3.3.5 Understanding the Estimators There are several ways to look at \\(b_1\\), each provides some insight from different perspectives. \\[ b_1 \\overset{(1)}{=} \\frac { \\color{blue}{ \\sum(X_i-\\bar{X})(Y_i-\\bar{Y}) } } { \\color{red}{\\sum(X_i-\\bar{X})^2} } \\overset{(2)}{=} r \\frac{s_Y}{s_X} \\overset{(3)}{=} \\color{blue}{\\sum} \\left( \\frac{ \\color{blue}{ (X_i-\\bar{X}) } }{ \\color{red}{SSX} } \\cdot \\color{blue}{(Y_i-\\bar{Y})} \\right) \\overset{(4)}{=} \\frac { \\color{blue}{Cov(X,Y)} } { \\color{red}{Var(X)} } \\] Note: Similar to \\(\\bar{X}\\), \\(SSX = (n-1)Var(X)\\) is treated as a constant and can therefore move in and out of the summation sign \\(\\sum\\) freely. When we change the scale of \\(X\\) to \\(cX\\) (without loss of generality, let’s assume \\(c\\ge1\\)), the term \\((X_i-\\bar{X})\\) would change by \\(c\\) and \\((X_i-\\bar{X})^2\\) would change by \\(c^2\\), and hence we would expect \\(b_1\\) to become \\(1/c \\cdot b_1\\). This result makes sense, since increase in the scale of X is canceled out by decrease in the coefficient of X such that their product remains the same. Similarly, when we change the scale of \\(Y\\) to \\(cY\\), we would expect \\(b_1\\) to become \\(c b_1\\). R_Proof: Linear Transformations x = rnorm(10000, mean=1, sd=5) c = 10 cx = c * x y = 2*x + rnorm(10000, 0, 5) cy = c * y dat = data.frame(x, cx, y, cy) b1 = lm( y ~ x, data=dat)$coefficients[2] b2 = lm( y ~ cx, data=dat)$coefficients[2] b3 = lm(cy ~ x, data=dat)$coefficients[2] bs = c(c,b1,b2,b3) names(bs) = c(&quot;c&quot;,&quot;x&quot;,&quot;cx&quot;,&quot;cy&quot;) bs ## c x cx cy ## 10.0000000 2.0020510 0.2002051 20.0205103 When X and Y are standardized z-scores, which entails that \\(s_X=1\\) and \\(s_Y=1\\), \\(b_1\\) becomes the Pearson correlation \\(r\\). \\[ b_1 = \\color{red}{r} \\cdot \\frac{s_Y}{s_X} = \\color{red}{ \\frac {\\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})} {s_X s_Y (n-1)} } \\cdot \\frac{s_Y}{s_X} = \\frac { \\sum_{i=1}^n(X_i - \\bar{X})(Y_i-\\bar{Y})} {(n-1)~ \\color{gold}{s_X^2} } = \\frac { \\sum(X_i-\\bar{X})(Y_i-\\bar{Y}) } { \\color{blue}{\\sum(X_i-\\bar{X})^2} } = b_1 \\] where \\[ \\color{gold}{s_X^2} = \\frac{ \\color{blue}{\\sum (X_i-\\bar{X})^2} }{n-1} \\] If we regard \\((X_i-\\bar{X})/SSX\\) as a weight, it is thus clear that \\(X_i\\) that is farther away from \\(\\bar{X}\\) has stronger influence on \\(\\hat{\\beta}_1\\). In fact, if a singular data point has undue (i.e., overly large) influence on regression coefficients, it is regarded as an influential data point. It should be clear that outliers are mostly likely to become influential data points. \\[ b_1 = \\sum \\left( \\color{red}{ \\frac{(X_i-\\bar{X})}{SSX} } \\cdot (Y_i-\\bar{Y}) \\right) \\overset{*}{=} \\sum \\left( \\color{red}{ \\frac{(X_i-\\bar{X})}{SSX} } \\cdot Y_i \\right) = \\sum \\left( \\color{red}{w_i} \\cdot Y_i \\right) \\] Note: To see how \\(\\overset{*}{=}\\) is true, let \\(c\\) be a constant and we have \\[ \\sum (X_i - \\bar{X})(Y_i \\pm c) = \\sum (X_i - \\bar{X})Y_i \\pm \\sum (X_i - \\bar{X})c = \\sum (X_i - \\bar{X})Y_i \\pm c\\sum (X_i - \\bar{X}) = \\sum (X_i - \\bar{X})Y_i \\pm c \\cdot 0 = \\sum (X_i - \\bar{X})Y_i \\] Measurement error in \\(X\\) always shrinks the magnitude of \\(b_1\\) towards 0, whereas measurement error in \\(Y\\) does not affect \\(b_1\\). To note, measurement error is regarded as random noise. To see this, let \\(X^* = X+E\\), where \\(E\\) denotes random error. \\[ Cov(X^*,Y) = Cov(X+E,~Y) = Cov(X,Y) + Cov(E,Y) = Cov(X,Y) \\] \\[ Var(X^*) = Var(X+E) = Var(X) + Var(E) + 2Cov(X,E) = Var(X) + Var(E) \\] Note: Both \\(Cov(Y,E)=0\\) and \\(Cov(X,E)=0\\) are true, because random noise by definition does not co-vary with anything. As a result, we have \\[ b_1^* = \\frac{Cov(X^*,Y)}{Var(X^*)} = \\frac{Cov(X+E,~Y)}{Var(X+E)} = \\frac{Cov(X,Y)}{Var(X)+Var(E)} &lt; \\frac{Cov(X,Y)}{Var(X)} = b_1\\] R_Proof: Measurement Error noise = rnorm(length(x), mean=0, sd=sd(x)) nx = x + noise noise = rnorm(length(y), mean=0, sd=sd(y)) ny = y + noise dat = data.frame(dat, nx, ny) b4 = lm( y ~ nx, data=dat)$coefficients[2] b5 = lm(ny ~ x, data=dat)$coefficients[2] names = names(bs) bs = c(bs, b4, b5) names(bs) = c(names,&quot;noise_x&quot;,&quot;noise_y&quot;) bs ## c x cx cy noise_x noise_y ## 10.0000000 2.0020510 0.2002051 20.0205103 0.9972068 2.0312171 Note: The result from adding noise to Y changes slightly, because the cor(Y,E) in our sample is 0.0132625, which is not strictly 0 due to sampling randomness. 3.3.6 Residual Variance Residual variance (aka conditional variance) is given as follows. \\[ \\hat{\\sigma}^2 = MSR = \\frac{SSR}{DFR} = \\frac{\\sum R_i^2}{df} = \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y_i})^2}{df} = \\frac{\\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2}{n-2} \\] It is thus clear that RMSE (i.e., root of mean squares residual) is the residual standard deviation, aka conditional standard deviation. Notation-wise, for the sake of simplicity, we will use \\(s\\) to denote the residual standard deviation \\(\\hat{\\sigma}\\) and hence \\(s^2\\) to represent the residual variance \\(\\hat{\\sigma}^2\\). 3.3.7 Standard Error of the Slope Given the residual standard deviation, \\[ \\hat{\\sigma} = \\sqrt{\\frac{\\sum R_i^2}{df}} = \\sqrt{ \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y_i})^2}{n-2} } = \\sqrt{ \\frac{\\sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2}{n-2} } \\] The standard errors of \\(b_1\\) and \\(b_0\\) are given as follows. \\[ \\hat{\\sigma}_{b_1}^2 = \\frac{\\hat{\\sigma}^2}{\\sum (X_i-\\bar{X})^2} = \\frac{\\hat{\\sigma}^2}{\\text{SSX}} = \\frac{\\hat{\\sigma}^2}{(n-1) Var(X)}\\] and \\[ \\hat{\\sigma}_{b_0}^2 = \\frac{\\hat{\\sigma}^2}{n} \\cdot \\frac{\\sum (X_i)^2}{\\sum (X_i-\\bar{X})^2} \\] It must be emphasized that we usually do not care about inference regarding \\(b_0\\). The standard error of \\(b_0\\) is provided here primarily for the sake of completeness rather than for any praticial value. It is clear that the standard error of \\(b_1\\) is directly proportional to the residual standard deviation. In other words, if the overall fit of the line is good, the RMSE (i.e., \\(\\hat{\\sigma}\\)) will be small and hence \\(\\hat{\\sigma}_{b_1}\\). When n is large, \\(\\hat{\\sigma}\\) would be small and \\(SSX\\) would be large, which drives \\(\\hat{\\sigma}_{b_1}\\) to become even smaller. All else being equal, greater intrinsic variation in X would result in a larger \\(SSX\\), which in turn produces a small \\(\\hat{\\sigma}_{b_0}\\). Results from surveys using a 1-5 Likert scale is a good example. Sometimes, the wording of a question is phrased in such a way that the responses to the question would only span a restricted range (e.g., registering exclusively positive ratings). This clearly leads to reduced intrinsic variation in X. 3.3.8 Effect Size and Standardized Coefficients Since the magnitude of \\(\\beta_0\\) and \\(\\beta_1\\) is subject to change with respect to the units of measurement adopted in Y and X. Therefore, it would be helpful to derive some kind of standardized coefficients such that sizes of the coefficients are invariant to the choice of measurement units. After standardizing both X and Y, the resultant coefficients are standardized coefficients, known as the beta coefficients. As shown previously, in simple OLS regression, standardized \\(b_1\\) is identical to the Pearson correlation \\(r\\). Beta coefficients are therefore used as an effect size measure in communicating the substantive practical significance of a linear relationship. Let \\(s_X\\) and \\(s_Y\\) be the standard deviations of X and Y, we have \\[ r = b_{\\text{standardized}} = \\frac{s_X}{s_Y} \\cdot b_{\\text{original}} \\] For certain problems, sometimes we would only want to standardize either X or Y, but not both of them. For example, when X is an ordinal variable measured on a 1-5 Likert scale. We might want to standardize only Y and claim that 1 point increase on the Likert scale would result in \\(b_1\\) standard deviations of increase in Y. 3.3.9 ANOVA for Regression HOWTO: how to add lines above and below the table? HOWTO: how to &quot;center&quot;&quot; content in a cell SS DF MS \\[\\mathbf{Total}~~\\] \\[~~\\sum(Y_i-\\bar{Y})^2~~\\] \\[~~n-1~~\\] \\[~~\\frac{1}{n-1} \\sum (Y_i-\\bar{Y})^2\\] \\[\\mathbf{Model}~~\\] \\[~~\\sum(\\hat{Y}_i-\\bar{Y})^2~~\\] \\[~~(1+p)-1~~\\] \\[~~\\frac{1}{p}\\sum (\\hat{Y}_i-\\bar{Y})^2\\] \\[\\mathbf{Residual}~~\\] \\[~~\\sum(Y_i - \\hat{Y}_i)^2~~\\] \\[~~n-(1+p)~~\\] \\[\\frac{1}{n-1-p}\\sum (Y_i-\\hat{Y}_i)^2\\] where \\(\\hat{Y}_i = b_0 + b_1X_i\\) and \\(p\\) is the number of features, i.e., the number of parameters minus 1 to exclude the intercept. \\[ R^2 = \\frac{\\text{SSM}}{\\text{SST}} = \\frac{ \\sum(\\hat{Y}_i-\\bar{Y})^2 }{ \\sum(Y_i-\\bar{Y})^2 } \\] and \\[ F = \\frac{MSM}{MSE} = \\frac{ \\frac{1}{p}\\sum (\\hat{Y}_i-\\bar{Y})^2 }{ \\frac{1}{n-1-p}\\sum (Y_i-\\hat{Y}_i)^2 } \\] where \\(F(p, n-1-p)\\) and \\(p=1\\) in the case of simple linear regression. TODO: check out p594 in IPS R_Proof: R-square and F-test mod = lm(mid3 ~ mid1, data=xdat) yhat = predict(mod) ybar = mean(xdat$mid3, na.rm=T) ssm = sum((yhat-ybar)^2, na.rm=T) sst = sum((xdat$mid3-ybar)^2, na.rm=T) r2 = ssm/sst r = cor(xdat$mid3, xdat$mid1, use=&quot;pairwise&quot;) # cor(na.omit(xdat$mid3), yhat) c(r2, r^2) ## [1] 0.5494976 0.5494976 3.4 Non-linear Relations 3.4.1 Log-transformations log-transformation is one way for handling non-linearity if the relation between IV and DV is indeed connected by a log-based relationship. log-transformation is commonly used for handling highly skewed variable by taknig advantage of the suppressive property of logarithm (e.g., \\(\\log_{10}(10)=1\\) and \\(\\log_{10}(100)=2\\)). As a result, many highly skewed variables, after log-transformations, would look much like a normal distribution. For simple linear regression, the linear model is represented as follows. \\[ Y = b_0 + b_1X + e \\] Take the derivative with respect to \\(X\\) on both sides, we have \\[ \\frac{dY}{dX} \\approx \\frac{\\Delta Y}{\\Delta X} = \\frac{d(b_0+b_1X)}{dX} = b_1 \\] and hence \\[ \\Delta Y = b_1 \\Delta X \\] In this case, the interpretation derived from a Calculus perspective readily agrees with that from an arithmetic perspective, i.e., when X increases by 1 unit (i.e., \\(\\Delta X=1\\)), Y would increase by an average of \\(b_1\\) (i.e., \\(\\Delta Y = b_1\\)). This situation would not be true under log-transformations. 3.4.1.1 Linear-log Model: Log-transformation on X Now, let’s consider log-transforming X. The linear model would become \\[ Y = b_0 + b_1 \\log(X) + e \\] To note, the log notation in this textbook would always denote the natural logarithm with base \\(e \\approx 2.718\\), which in some textbooks is represented by ln. Simple rules of arithmetic dictate that \\(b_1\\) is the expected units of change in Y given a one-unit increase in log(X), which means \\[ \\log(X) + 1 = \\log(X) + \\log(e) = \\log(eX) \\] This result makes it clear that adding 1 to log(X) is equivalent of multipling X by \\(e \\approx 2.718\\). In other words, Y is expected to change by \\(b_1\\) units when X increases by approximately 1.72 times or 172%. Let’s look at this from a Calculus perspective by taking the derivative with respect to X on the both sides. \\[ \\frac{dY}{dX} \\approx \\frac{\\Delta Y}{\\Delta X} = \\frac{d(b_0+b_1 \\log(X))}{dX} = b_1 \\cdot \\frac{1}{X} \\] and hence \\[ \\Delta Y = b_1 \\cdot \\frac{\\Delta X}{X} = \\frac{b_1}{100} \\cdot \\left( 100 \\cdot \\frac{\\Delta X}{X} \\right) = \\frac{b_1}{100} \\cdot p \\] where \\(\\Delta X/X\\) represents the proportion of change in X and hence \\(100 \\cdot \\Delta X/X\\) denotes the percentage of change in X. In other words, 1% increase in X, namely \\((100 \\cdot \\Delta X / X) = 1\\), would translate into \\(b_1/100\\) units of increase in \\(Y\\) or one hundredth of Y. To note, this Calculus-inspired percentage interpretation is only true (i.e., the \\(\\approx\\) sign would hold), if \\(\\Delta X\\) is small (e.g., 1%). Now let’s re-examine this interpretation from the arithmetic perspective, where a p percent increase in X would be \\(X \\cdot (100+p)/100 = X \\cdot (1+p/100)\\). Let \\(Y_0\\) and \\(Y_p\\) be the values of Y corresponding to X and X with p percent increase. \\[ Y_0 = b_0 + b_1 \\log(X) + e \\] and \\[ Y_p = b_0 + b_1 \\log \\left( X \\cdot \\frac{100+p}{100} \\right) + e \\] and \\[ \\frac{\\Delta X}{X} = \\frac{X \\cdot (1+p/100) - X}{X} = \\frac{p}{100} \\] or \\[ 100 \\cdot \\frac{\\Delta X}{X} = p \\] Hence, a p percent increase in X would result in \\[ \\Delta Y = Y_p - Y_0 = b_1 \\left( \\log \\left( (1+\\frac{p}{100}) \\cdot X \\right) - \\log(X) \\right) = b_1 \\log \\left( 1 + \\frac{p}{100} \\right) \\] This result indicates that p = 1 or 1% increase in X would result in \\(b_1 \\log(1.01) \\approx b_1 \\cdot 0.01\\). This interpretation is scalable up to about 10% (i.e., the exact arithmetic and the approximate Calculus interpretations agree only up to about 10%), since p = 10 or 10% increase in X would result in \\(b_1 \\log(1.10) \\approx b_1 \\cdot 0.10\\). The following table shows the degree of deviations under and beyond 10% change in X. proportion increase in X percentage increase in X increase in Y \\(\\Delta X/X = p/100\\) \\(100 \\cdot \\Delta X/X = p\\) \\(b_1 \\log(1+p/100)\\) 0.01 1% \\(b_1 \\cdot 0.01\\) 0.05 5% \\(b_1 \\cdot 0.05\\) 0.10 10% \\(b_1 \\cdot 0.10\\) 0.20 20% \\(b_1 \\cdot 0.18\\) 0.50 50% \\(b_1 \\cdot 0.41\\) 1.00 100% \\(b_1 \\cdot 0.69\\) 1.72 172% \\(b_1 \\cdot 1.00\\) TODO: add a shiny app to supplement the table! curve(log, from=0.01, to=4, ylim=c(-4,4)) curve(x-1, add=T, col=2) abline(v=0, h=0, lty=2) In summary, given the discussions above, the following statements are equivalent (or almost equivalent). When log(X) increases by 1 unit, Y would change by an average of \\(b_1\\). When X is multiplied by 2.72, Y would change by an average of \\(b_1\\). When X increases by 172%, Y would change by an average of \\(b_1\\). When X increases by p%, Y would change by an average of \\(b_1 \\log(1+p/100)\\). When X increases by 1%, Y would change by an average of \\(b_1/100\\). When X increases by 10%, Y would change by an average of \\(b_1/10\\). 3.4.1.2 Log-linear Model: Log-transformation on Y \\[ \\log(Y) = b_0 + b_1X + e \\] Take the derivative with respect to X on both sides, we have \\[ \\frac{1}{Y} \\cdot \\frac{dY}{dX} \\approx \\frac{1}{Y} \\cdot \\frac{\\Delta Y}{\\Delta X} = b_1 \\] and hence \\[ \\frac{\\Delta Y}{Y} = b_1 \\cdot \\Delta X \\] Replacing proportion of change by percentage of change gives \\[ 100 \\cdot \\frac{\\Delta Y}{Y} = 100 \\cdot b_1 \\cdot \\Delta X \\] which leads to the interpretation that 1 unit increase in X would result in \\(100 \\cdot b_1\\) percent increase in Y. Similar to the previous case, this Calculus interpretation is only true when \\(b_1\\) is less than 0.1 or 10%. Let’s examine the precise arithmetic interpretation. From \\(\\log(Y) = b_0 + b_1X + e\\), we have \\[ Y = e^{(b_0 + b_1X + e)} \\] Let \\(Y_0\\), \\(Y_1\\), and \\(Y_p\\) be the values of Y corresponding to X, X+1, and X+p. We have \\[ Y_0 = e^{(b_0 + b_1X + e)} \\] and \\[ Y_1 = e^{(b_0 + b_1(X+1) + e)} = e^{(b_0 + b_1X + e)} \\cdot e^{b_1} = Y_0 \\cdot e^{b_1} \\] which implies that 1 unit increase in X would result in Y multiplied by \\(e^{b_1}\\). \\[ \\frac{\\Delta Y}{Y_0} = \\frac{Y_1 - Y_0}{Y_0} = \\frac{Y_0 \\cdot e^{b_1} - Y_0}{Y_0} = \\frac{Y_0 \\cdot (e^{b_1} - 1)}{Y_0} = (e^{b_1}-1) \\] Replacing proportion of change by percentage of change gives \\[ 100 \\cdot \\frac{\\Delta Y}{Y_0} = 100 \\cdot (e^{b_1}-1) \\] The Taylor expansion of \\(e^b\\) gives the following result \\[ e^b = 1 + b + \\frac{b^2}{2!} + \\frac{b^3}{3!} + \\cdots+\\frac{b^n}{n!} \\] When b is small (e.g. \\(b \\le 0.1\\)), \\(e^b \\approx 1 + b\\) and hence \\(e^b - 1 \\approx b\\). In other words, when \\(b_1\\) is less than 0.1 (as suggested by the table below), the exact arithmetic interpretation agrees with the approximate Calculus interpretation. b = c(c(0.01,0.05,0.1,0.15), seq(0.2, 1, by=0.1)) p = round(exp(b)-1, 2) mat = cbind(b,p) colnames(mat) = c(&quot;b&quot;,&quot;exp(b)-1&quot;) mat ## b exp(b)-1 ## [1,] 0.01 0.01 ## [2,] 0.05 0.05 ## [3,] 0.10 0.11 ## [4,] 0.15 0.16 ## [5,] 0.20 0.22 ## [6,] 0.30 0.35 ## [7,] 0.40 0.49 ## [8,] 0.50 0.65 ## [9,] 0.60 0.82 ## [10,] 0.70 1.01 ## [11,] 0.80 1.23 ## [12,] 0.90 1.46 ## [13,] 1.00 1.72 In summary, When X increases by 1, log(Y) would increase by \\(b_1\\). When X increases by 1, Y would be multiplied by \\(e^{b_1}\\). When X increases by c, Y would be multiplied by \\(e^{c b_1}\\). When X increases by 1, Y would increase by \\(100 \\cdot (e^{b_1}-1)\\) percent. When X increases by 1 and when \\(b_1 \\le 0.1\\), Y would increase by \\(100 \\cdot b_1\\) percent. 3.4.1.3 Log-log Model: log-transformations on X and Y \\[ \\log(Y) = b_0 + b_1 \\log(X) + e \\] Taking the derivative with respect to X on both sides gives \\[ \\frac{1}{Y} \\cdot \\frac{dY}{dX} \\approx \\frac{1}{Y} \\cdot \\frac{\\Delta Y}{\\Delta X} = \\frac{b_1}{X} \\] and hence \\[ 100 \\cdot \\frac{\\Delta Y}{Y} = b_1 \\cdot \\left( 100 \\cdot \\frac{\\Delta X}{X} \\right) \\] which implies that 1 percent increase in X would result in \\(b_1\\) percent increase in Y. With the log-log model, the coefficient \\(b_1\\) is called elasticity in economics. In summary, When log(X) increase by 1, log(Y) would increase by \\(b_1\\). When X is multiplied by \\(e\\), the expected value of Y would increase by multiplying \\(e^{b_1}\\). When X increases by p percent, the expected value of Y would increase by multiplying \\(e^{c b_1}\\) where \\(c = \\log(1+p/100)\\). 3.4.2 Quadratic Relations 3.5 The Assumptions of Simple Linear Regression When the OLS assumptions hold, OLS estimators have been shown to be unbiased and have minimum standard error of among all unbiased linear estimators, a property known as Best Linear Unbiased Estimators (BLUE). \\[ Y_i|X_i \\overset{id}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2) \\] 3.6 Assumption Diagnostics "],
["multiple-linear-regression.html", "4 Multiple Linear Regression 4.1 Regression Models - The Base Form 4.2 The Ordinary Least-Squres Method 4.3 Standard Errors of Regression Coefficients 4.4 \\(R^2\\) and ANOVA for Multiple Regression 4.5 Regression Models Extended - Categorical Features 4.6 Regression Models Extended - Interaction Effects 4.7 Regression Models Extended - Nonlinear Relations 4.8 Model Comparision &amp; Evaluation", " 4 Multiple Linear Regression Multiple linear regression, or simply multiple regression, is a powerful statistical modeling technique that can handle independent variables of assorted data types, i.e., ditchotomous, nominal, continuous complex relations among the IVs, e.g., nonlinear relations and interaction effects 4.1 Regression Models - The Base Form 4.1.1 Model Specification with Population Parameters In a simple linear regression model, we use only one independent variable X to predict a continuous dependent variable Y and the prediction model has two parameters, i.e. the intercept and the slope. In this chapter, instead of usign one IV, we will use p IVs, i.e. \\(X_1, X_2, \\dots, X_p\\). As a result, with the population parameters in place, the multiple linear regression model is \\[ Y_i = \\mu_i + \\varepsilon_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} + \\varepsilon_i \\] where \\(i = 1,2,\\dots,n\\) and \\(\\mu_i \\equiv E(Y_i|\\boldsymbol{X}_i)\\) is commonly referred to as the mean response model or the prediction model \\[ E(Y_i|\\boldsymbol{X}_i) \\equiv \\mu_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} \\] and \\(\\varepsilon_i\\) is called the error and \\[\\varepsilon_i \\overset{iid}{\\text{~}} N(0,\\sigma^2)\\] To note, the population parameters of this linear model are \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) and \\(\\sigma\\). 4.1.2 Model Specification with Sample Parameters Accordingly, using the sample parameters, the multiple linear regression model is \\[ Y_i = \\hat{Y}_i + e_i = b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi} + e_i \\] where \\(i = 1,2,\\dots,n\\) and the prediction model is \\[ \\hat{Y}_i = b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi} \\] and \\(e_i\\) is called the residual and \\[ e_i \\overset{iid}{\\text{~}} N(0,s^2)\\] To note, \\(b_0, b_1, \\dots, b_p\\) and \\(s^2\\) are parameters estimated from a given sample to approximate the unknown and constant population parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) and \\(\\sigma\\). 4.2 The Ordinary Least-Squres Method Similar to the case of simple linear regression, the ordinary least squares (OLS) method can also be applied to estimate sample parameters for multiple linear regression. \\[ SSR = \\sum_{i=1}^n R_i^2 = \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - (b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi}))^2 \\] The solutions for \\(b_j\\), \\(j = 1,2,\\dots,p\\), should minimize \\(SSR\\), hence the name least-squares method. 4.2.1 Interpretations of Regression Coefficients Given the following mean response model, \\[ \\hat{Y}_i = b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi} \\] intercept \\(b_0\\): The expected value (i.e., mean) of Y when all IVs are zero. slope \\(b_i\\): Holding other IVs constant, increase in \\(X_i\\) by 1 unit corresponds to an average of \\(b_i\\) units of increase in Y. 4.2.2 Residual Variance The residual variance (aka conditional variance) is given as follows. \\[ s^2 = MSR = \\frac{\\sum R_i^2}{df} = \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y_i})^2}{df} = \\frac{\\sum_{i=1}^n (Y_i - (b_0 + b_1X_{1i} + b_2X_{2i} + \\cdots + b_pX_{pi}))^2}{n-(1+p)} \\] It should be clear that \\(1+p\\) is the number of parameters required to estimate \\(\\hat{Y}_i\\) (i.e., the degrees of freedom we have to spend or give up for fitting the model), which equals \\(p\\) slope parameters plus 1 for the intercept. The denominator \\(n-(1+p)\\) is the degrees of freedom that are left for us after fitting the model. As a result, \\(s\\) is called the residual standard deviation or conditional standard deviation or RMSE. 4.3 Standard Errors of Regression Coefficients As explained previously, we are usually only interested in the standard error of a slope. Let \\(b_j\\) denote the slope parameter for the jth feature \\(X_j\\) where \\(j = 1,2,\\dots,p\\). We have \\[ s_{b_j}^2 = \\frac{s^2}{\\text{SSX}_j} \\cdot \\text{VIF}_j = \\frac{s^2}{(n-1) Var(X_j)} \\cdot \\text{VIF}_j \\] where \\(s^2\\) is the residual variance, \\(\\text{SSX}_j = \\sum_{i=1}^m (X_{ij}-\\bar{X}_j)^2\\) is the sum of squares for \\(X_j\\), and \\(\\text{VIF}_j\\) is an important concept called variance inflation factor. 4.3.1 Variance Inflation Factor To compute VIF for \\(X_j\\), use the following equation. \\[ \\text{VIF}_j = \\frac{1}{1-R_j^2} \\] where \\(R_j^2\\) is the R-squared of regressing \\(X_j\\) on all the other IVs except for \\(X_j\\). To be clear, the VIF of \\(X_j\\) can be computed in three steps. Step 1: For \\(X_1,X_2,\\dots,X_p\\), regress \\(Y_j\\) on the rest of the IVs: There are only \\(p-1\\) features and \\(Y\\) is completely ignored. \\[ X_j = a_0 + a_1X_1 + a_2X_2 + \\cdots + a_pX_p \\] Step 2: Compute the \\(R^2\\) for this regression model, which results in \\(R_j^2\\). Step 3: Compute VIF based on the formula given above. 4.3.2 Significance of VIF Remember that \\(R^2\\) is interpreted as the proportion of variance explained by the IVs and \\(1-R^2\\) is the proportion of unexplained variance. It should be clear that when \\(X_j\\) highly correlates with other features, \\(R_j^2\\) would be high and \\(1-R_j^2\\) would be low and hence \\(VIF_j\\) tends to be high. In fact, when \\(X_j\\) does not correlate with any other features, i.e., \\(R_j^2 = 0\\), we will have \\(VIF_j = 1\\), which would give the smallest possible standard error for \\(b_j\\), as shown in the following equation. To note, this is the same formula for the standard error of \\(b_j\\) in simple linear regression. \\[ s_{b_j}^2 = \\frac{s^2}{\\text{SSX}_j} \\cdot \\text{VIF}_j = \\frac{s^2}{\\text{SSX}_j} \\] In contrast, if \\(X_j\\) highly correlates with some other features such that the \\(R_j^2 = 0.90\\), i.e., 90% of the variance in \\(X_j\\) can be explained by the other features, then \\(VIF_j = 10\\). In other words, the standard error of \\(b_j\\) derived from multiple linear regression would be inflated 10 times as compared to the standard error of \\(b_j\\) that we would have got from a simple linear regression model where \\(X_j\\) is the sole predictor. 4.3.3 Multicollinearity This previous example illustrates how the term VIF gets its name variance inflation factor and the phenonemon of variance inflation due to presence of highly correlated featuers is called multicollinearity. As a rule of thumb, multicollinearity is usually not regarded as an issue unless VIF is greater than 10. Another point worth noticing is that, since in most situations, a given feature will always correlate with some other features, IVs or features in the context of multiple linear regression are also called covariates (since features co-vary), which is particularly true for continuous features. 4.3.4 Influencing Factors on Standard Errors With the meaning of VIF explained, we can now discuss the factors affecting the standard error of the slope parameter \\(b_j\\) for the covariate \\(X_j\\). \\[ s_{b_j}^2 = \\frac{s^2}{\\text{SSX}_j} \\cdot \\text{VIF}_j = \\frac{s^2}{(n-1) Var(X_j)} \\cdot \\frac{1}{1-R_j^2} \\] \\(s^2\\): Better model fit leads to smaller standard errors, or greater scatter in data around the regression line leads to more uncertainties in slope parameter estimations. \\(n\\): Larger sample size results in smaller standard errors. \\(Var(X_j)\\): Greater instrinsic variability in a covariate yields smaller standard errors. \\(R_j^2\\): Features that highly correlate with other features might associate with large standard errors. 4.4 \\(R^2\\) and ANOVA for Multiple Regression When dealing with simple linear regression, we have already studied how to derive the ANOVA table, compute the \\(R^2\\) and perform the \\(F\\) test. The same results also hold here, which will not be repeated except for one point. The formula for computing \\(R^2\\) is \\[ R^2 = \\frac{\\text{SSM}}{\\text{SST}} = \\frac{ \\sum(\\hat{Y}_i-\\bar{Y})^2 }{ \\sum(Y_i-\\bar{Y})^2 } \\] In the context of multiple regression, the statistic \\(R^2\\) is called the coefficient of determination of the linear model, and the square root of \\(R^2\\) is called the multiple correlation coefficient, which interestingly enough is the correlation between observed values \\(Y_i\\) and predicted values \\(\\hat{Y}_i\\). R_Proof: Multiple Correlation Coefficient tdat = na.omit(xdat[c(&quot;mid3&quot;,&quot;satmath&quot;,&quot;satreading&quot;,&quot;gpa&quot;)]) mod1 = lm(mid3 ~ satmath + satreading + gpa, data=tdat) yhat = predict(mod1) ybar = mean(tdat$mid3, na.rm=T) ssm = sum((yhat-ybar)^2, na.rm=T) sst = sum((tdat$mid3-ybar)^2, na.rm=T) r2 = ssm/sst r = cor(tdat$mid3, yhat) c(sqrt(r2), r) ## [1] 0.07316646 0.07316646 4.5 Regression Models Extended - Categorical Features 4.5.1 Dummy Variables A dummy variable has only two unique levels. For comparison purposes, we would designate one level as the reference level and code it as 0, whereas the other level is coded as 1. Since 1 is usually used to indicate the presence of something of interest, a dummy variable is also called a indicator variable. When a dummy variable is the only feature in a linear regression model, it gives identical results to a two-sample t-test. To see how this is true, let’s use a concrete example, where we regress final exam scores on gender to see if males outperformed females in a Chemistry class. Let us use M to indicate a dummy variable, where M=0 for females and M=1 for males. The prediction model would be as follows. \\[ \\hat{Y}_i = b_0 + b_1M_i \\] Three points can be inferred from this equation: When \\(M_i = 0\\), the group average for females is \\(\\hat{Y}_i=b_0\\). When \\(M_i = 1\\), the group average for males is \\(\\hat{Y}_i=b_0+b_1\\). The interpretation of \\(b_1\\) is that 1 unit increase in \\(M_i\\), i.e., changing from female to male, \\(\\hat{Y}_i\\) on average would increase by \\(b_1\\). Putting together, it should be clear that (a) \\(b_0\\) is the group mean for females, (b) \\(b_0+b_1\\) is the group mean for males, and (c) \\(b_1\\) is the difference in group means. R_Proof: Regression vs Two-sample T-test # two-sample t-test with equal variance tt = t.test(mid3 ~ gender, data=xdat, var.equal=TRUE) # simple linear regression with a dummy feature mod = lm(mid3 ~ gender, data=xdat) modsum = summary(mod) modsum$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 98.426230 3.720447 26.455485 1.282478e-65 ## genderMale 8.830913 6.161646 1.433207 1.534421e-01 c(diff(tt$estimate), tt$statistic, tt$p.value) ## mean in group Male t ## 8.8309133 -1.4332070 0.1534421 r2 = modsum$r.squared r = cor(xdat$mid3, as.numeric(xdat$gender)-1, use=&quot;pair&quot;) c(r2, r^2) ## [1] 0.01069533 0.01069533 4.5.2 Nominal Variables In regression analysis, a nominal variable is routinely converted into a set of dummy features, designate one level/feature as the reference level, and put the rest of the levels/features into the regression model. To illustrate this, let’s regress final exam scores on year to examine if students from different years performed differently. There are three sessions (i.e., years) in the dataset. Let’s use S1, S2, and S3 to denote the first, second, and third sessions, and designate S1 as the reference level. The prediction model would be as follows. \\[ \\hat{Y}_i = b_0 + b_1S_{2i} + b_2S_{3i} \\] To note, since the dummy feature S1 is used as the reference level, it is absent as a feature from the equation. Moreover, the three dummy variables are mutually exlucsive, e.g., when S1=1, S2 and S3 have to be zero. Three points can be inferred from this equation: When \\(S_{1i} = 1\\), the group average for students from the first session is \\(\\hat{Y}_i=b_0\\). When \\(S_{2i} = 1\\), the group average for students from the second session is \\(\\hat{Y}_i=b_0+b_1\\). When \\(S_{3i} = 1\\), the group average for students from the third session is \\(\\hat{Y}_i=b_0+b_2\\). Putting together, it should be clear that (a) \\(b_1\\) is the difference in group means between the second and first sessions, and (b) \\(b_2\\) is the difference in group means between the third and first sessions. If there are more sessions up to p levels, then \\(b_3,b_4,\\dots,b_p\\) would be difference in group means between each of these levels and the first session. Everything is compared to the first session, which is why it is termed as the reference level or base level or reference group. When a nominal variable is converted into a set of dummies and used as the only feature in a linear regression model, it gives identical results to the one-way ANOVA. R_Proof: Regression vs ANOVA # one-way ANOVA aovmod = aov(mid3 ~ session, data=xdat) aovsum = summary(aovmod) aovsum ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## session 2 114 56.8 0.033 0.967 ## Residuals 189 324206 1715.4 ## 8 observations deleted due to missingness # simple linear regression with a nominal feature mod = lm(mid3 ~ session, data=xdat) anova(mod) ## Analysis of Variance Table ## ## Response: mid3 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## session 2 114 56.81 0.0331 0.9674 ## Residuals 189 324206 1715.38 modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ session, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -86.620 -32.910 -0.779 32.221 89.380 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 100.571 5.535 18.171 &lt;2e-16 *** ## sessionSS10 2.049 8.058 0.254 0.800 ## sessionSS11 1.208 7.112 0.170 0.865 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 41.42 on 189 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.0003503, Adjusted R-squared: -0.01023 ## F-statistic: 0.03312 on 2 and 189 DF, p-value: 0.9674 modsum$fstatistic ## value numdf dendf ## 0.0331181 2.0000000 189.0000000 Let’s create a nominal feature with three levels (i.e., ‘a’,‘b’,‘c’) and use the model.matrix() function in R to generate a set of dummy features. xvec = gl(3, 2, labels=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) tdat = data.frame(xvec) tdat ## xvec ## 1 a ## 2 a ## 3 b ## 4 b ## 5 c ## 6 c model.matrix(~xvec, data=tdat) ## (Intercept) xvecb xvecc ## 1 1 0 0 ## 2 1 0 0 ## 3 1 1 0 ## 4 1 1 0 ## 5 1 0 1 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$xvec ## [1] &quot;contr.treatment&quot; Notice that (a) a column of 1s is automatically created, and (b) the first level ‘a’ is missing from the results. model.matrix(~xvec-1, data=tdat) ## xveca xvecb xvecc ## 1 1 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 1 0 ## 5 0 0 1 ## 6 0 0 1 ## attr(,&quot;assign&quot;) ## [1] 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$xvec ## [1] &quot;contr.treatment&quot; 4.6 Regression Models Extended - Interaction Effects The interaction effects are also called moderation effects, which are critically important in regression analysis. It is the idea that the effect of X on Y is moderated by Z. For example, we might be interested in the effect of study time X on course performance Y. It is conceivable that the way study time affects exam results might depend on whether the time is spent willingly Z. If students are forced to spend a lot of time studying, then the more time spent may or may not translate to stronger performance. If students are willingly investing time in a course, more time spent should have a much stronger effect on exam performance. 4.6.1 Dummy-Dummy Interactions In our dataset, let’s consider the effect of repeater status R (i.e., repeater=1) on final exam scores Y moderated by gender Male (i.e., male=1). \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{R} + b_2\\cdot\\text{Male} + b_3\\cdot\\text{R}\\cdot\\text{Male} \\] For \\(Male=0\\) (i.e., females), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{R} \\] which shows (a) that the average score for female non-repeaters is \\(b_0\\) and (b) that females enjoyed an average of \\(b_1\\) points increase when switching from non-repeater to repeater status. For \\(Male=1\\) (i.e., males), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{R} + b_2 + b_3\\cdot\\text{R} = (b_0+b_2) + (b_1+b_3)\\cdot\\text{R}\\] which shows (a) that the average score for male non-repeaters was \\(b_2\\) points higher than that for female non-repeaters and (b) that for the same switch from non-repeater to repeater status, males enjoyed an additional \\(b_3\\) points increase as compared to the score gain for females making the same status switch. Now let’s fit a linear regression model and examine the coefficients. mod = lm(mid3 ~ repeater*gender, data=xdat) modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ repeater * gender, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -91.109 -26.337 2.514 24.455 80.976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.024 3.861 28.752 &lt; 2e-16 *** ## repeaterRepeater -41.537 7.012 -5.924 1.47e-08 *** ## genderMale 13.085 6.516 2.008 0.0461 * ## repeaterRepeater:genderMale -7.613 11.381 -0.669 0.5043 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.6 on 188 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.2653, Adjusted R-squared: 0.2536 ## F-statistic: 22.63 on 3 and 188 DF, p-value: 1.488e-12 The visualize a dummy-continuous interaction effect, fit a simple linear regression model between X and Y conditioned on Z. newdat = expand.grid(mod$xlevels) newdat$yhat = predict(mod, newdat) yrange = range(newdat$yhat) yrange = yrange + c(-1,1)*diff(yrange)*0.1 plot(1:2, newdat$yhat[1:2], ylim=yrange, type=&quot;o&quot;, xaxt=&quot;n&quot;, ylab=&quot;Final Exam Scores&quot;, xlab=NA) axis(1, at=1:2, labels=levels(newdat[,1])) points(1:2, newdat$yhat[3:4], type=&quot;o&quot;, pty=2, lty=2) 4.6.2 Dummy-Continuous Interactions In our dataset, let’s consider the effect of high school GPA X on final exam scores Y moderated by repeater status R. \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} + b_2\\cdot\\text{R} + b_3\\cdot\\text{GPA}\\cdot\\text{R} \\] For \\(R=0\\) (i.e., non-repeaters), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} \\] which shows that (a) a non-repeater with 0 highschool GPA got an average of \\(b_0\\) points in the final exam, and (b) that the average effect of 1 point increase in GPA on exam performance is \\(b_1\\) points for non-repeaters. For \\(R=1\\) (i.e., repeaters), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} + b_2 + b_3\\cdot\\text{GPA} = (b_0+b_2) + (b_1+b_3)\\cdot\\text{GPA}\\] which shows that (a) repeaters with 0 highschool GPA would on average get \\(b_2\\) points higher on the final exam, and (b) that the average effect of 1 point increase in GPA on exam performance is \\(b_1+b_3\\) for repeaters. Hence, for the same 1 point increase in GPA, repeaters enjoyed an extra \\(b_3\\) points increase in the final exam as compared to the \\(b_1\\) points gain in final scores for non-repeaters. The visualize a dummy-continuous interaction effect, fit a simple linear regression model between X and Y conditioned on Z. car::scatterplot(mid3 ~ gpa*repeater, data=xdat) In R, interaction effect between X and Z is represented by X*Z. To note, whenever an interaction effect (e.g., X*Z) is added to a linear model, the component variables involved (e.g., X and Z) will be automatically added to the linear model as well. In other words, the following two linear models give identical results. lm(mid3 ~ repeater*gpa, data=xdat) lm(mid3 ~ repeater + gpa + repeater*gpa, data=xdat) Now let’s fit a linear regression model and examine the coefficients. mod = lm(mid3 ~ repeater*gpa, data=xdat) modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ repeater * gpa, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -83.477 -26.153 4.577 24.992 77.077 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 129.451 15.230 8.499 5.79e-15 *** ## repeaterRepeater -99.104 29.478 -3.362 0.000938 *** ## gpa -4.738 5.107 -0.928 0.354678 ## repeaterRepeater:gpa 18.394 9.668 1.903 0.058618 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.67 on 188 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.2624, Adjusted R-squared: 0.2507 ## F-statistic: 22.3 on 3 and 188 DF, p-value: 2.135e-12 4.6.3 Continuous-Continuous Interactions Let’s consider the effect of highschool GPA on final exam scores Y moderated by perceived usefulness of the course U. \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} + b_2 \\cdot U + b_3\\cdot\\text{GPA} \\cdot U \\] When \\(U=0\\), we have \\[ \\hat{Y}_i = b_0 + b_1\\cdot\\text{GPA} \\] When \\(U=1\\), we have \\[ \\hat{Y}_i = (b_0 +b_2) + (b_1+b_3)\\cdot\\text{GPA} \\] At this point, the interpretations of the coefficients are very similar to those we have encountered in the case of dummy-continuous interactions. To be noted, if we use \\(U\\) and \\(U+1\\) instead of using \\(U=0\\) and \\(U=1\\), we would end up with very similar interpretations: (a) 1 unit increase in perceived course usefulness would result in \\(b_0\\) points increase in final exam scores for students with 0 GPA, and (b) 1 unit increase in perceived course usefulness would enhance the effect of GPA by \\(b_3\\) points per 1 unit increase in GPA. mod = lm(mid3 ~ gpa*utcourse, data=xdat) modsum = summary(mod) modsum ## ## Call: ## lm(formula = mid3 ~ gpa * utcourse, data = xdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -88.790 -31.017 1.257 30.518 91.027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.479 57.140 0.848 0.397 ## gpa 13.523 19.300 0.701 0.484 ## utcourse 13.042 12.221 1.067 0.287 ## gpa:utcourse -3.354 4.090 -0.820 0.413 ## ## Residual standard error: 41.6 on 171 degrees of freedom ## (25 observations deleted due to missingness) ## Multiple R-squared: 0.01214, Adjusted R-squared: -0.005192 ## F-statistic: 0.7004 on 3 and 171 DF, p-value: 0.553 xs = quantile(xdat$gpa, na.rm=TRUE) ys = quantile(xdat$utcourse, na.rm=TRUE, probs=c(0.25, 0.75)) newdat = expand.grid(xs,ys) colnames(newdat) = c(&quot;gpa&quot;,&quot;utcourse&quot;) newdat$yhat = predict(mod, newdat) yrange = range(newdat$yhat) yrange = yrange + c(-1,1)*diff(yrange)*0.1 plot(xdat$gpa, xdat$mid3,xlab=&quot;GPA&quot;, ylab=&quot;Final Exam Scores&quot;) points(newdat$gpa[1:5], newdat$yhat[1:5], type=&quot;o&quot;, pch=2) points(newdat$gpa[6:10], newdat$yhat[6:10], type=&quot;o&quot;, pch=5, lty=2) legend(&quot;topleft&quot;, legend=c(&quot;25% Quantile&quot;,&quot;75% Quantile&quot;), bty=&quot;n&quot;, pch=c(2,5), lty=c(1,2)) To note, do not plot only the fitted lines without the actual data points. plot(newdat$gpa[1:5], newdat$yhat[1:5], ylim=yrange, type=&quot;o&quot;, xlab=&quot;GPA&quot;, ylab=&quot;predicted&quot;, pch=2) points(newdat$gpa[6:10], newdat$yhat[6:10], type=&quot;o&quot;, pch=5, lty=2) 4.7 Regression Models Extended - Nonlinear Relations 4.8 Model Comparision &amp; Evaluation 4.8.1 F-test for Nested Models 4.8.2 Adjusted R-Squared 4.8.3 Information Criteria "],
["key-formatting-constructs.html", "5 Key Formatting Constructs 5.1 Emphasis 5.2 Superscripts 5.3 Lists 5.4 Block Quotes 5.5 Displaying Blocks of Code Without Evaluating 5.6 Displaying R Code Inline in a Sentence 5.7 Evaluating and Inserting R Code in a Sentence 5.8 Typesetting Equations 5.9 Inline vs. Display Material 5.10 Some LaTeX Basics", " 5 Key Formatting Constructs The key formatting constructs are discussed at http://rmarkdown.rstudio.com/authoring_basics.html. You can see above how I constructed main section headings. Now I’m going to create a series of sections using secondary headings. 5.1 Emphasis This is italic. This is bold. 5.2 Superscripts This is y2. 5.3 Lists 5.3.1 Unordered Item 1 Item 2 Item 2a Item 2b 5.3.2 Ordered Item 1 Item 2 Item 3 Item 3a Item 3b Item 1 Item 2 Item 3 Item 3a Item 3b 5.4 Block Quotes A friend once said: It’s always better to give than to receive. 5.5 Displaying Blocks of Code Without Evaluating In some situations, you want to display R code but not evaluate it. Here is an example of how you format. This text is displayed verbatim / preformatted 5.6 Displaying R Code Inline in a Sentence Sometimes, you need to include an R name or command inline in a sentence. Here is how your format it. The sqrt function computes the square root of a number. 5.7 Evaluating and Inserting R Code in a Sentence Sometimes, you want a result without showing the user that you used R to get it. Here is an example. The mean of the numbers 2,3,4 is 3. There are lots of ways you can exploit this capability. You can do more advanced calculations in a hidden code block, assign the results to variables, and then simply use the variable names to insert the results in a sentence. In the following example, I compute the sum of the integers from 2 to 19 in a hidden code block. Then I display the result inline. The sum of the integers from 2 to 19 is 189. 5.8 Typesetting Equations 5.9 Inline vs. Display Material Equations can be formatted inline or as displayed formulas. In the latter case, they are centered and set off from the main text. In the former case, the mathematical material occurs smoothly in the line of text. In order to fit neatly in a line, summation expressions (and similar constructs) are formatted slightly differently in their inline and display versions. Inline mathematical material is set off by the use of single dollar-sign characters. Consequently, if you wish to use a dollar sign (for example, to indicate currency), you need to preface it with a back-slash. The following examples, followed by their typeset versions, should make this clear This summation expression $\\sum_{i=1}^n X_i$ appears inline. This summation expression \\(\\sum_{i=1}^n X_i\\) appears inline. This summation expression is in display form. $$\\sum_{i=1}^n X_i$$ This summation expression is in display form. \\[\\sum_{i=1}^n X_i\\] 5.10 Some LaTeX Basics In this section, we show you some rudiments of the LaTeX typesetting language. 5.10.1 Self-Sizing Parentheses In LaTeX, you can create parentheses, brackets, and braces which size themselves automatically to contain large expressions. You do this using the \\left and \\right operators. Here is an example $$\\sum_{i=1}^{n}\\left( \\frac{X_i}{Y_i} \\right)$$ \\[\\sum_{i=1}^{n}\\left( \\frac{X_i}{Y_i} \\right)\\] 5.10.2 Special Functions LaTeX typesets special functions in a different font from mathematical variables. These functions, such as \\(\\sin\\), \\(\\cos\\), etc. are indicated in LaTeX with a backslash. Here is an example that also illustrates how to typeset an integral. $$\\int_0^{2\\pi} \\sin x~dx$$ \\[\\int_0^{2\\pi} \\sin x~dx\\] 5.10.3 Matrices Matrics are presented in the array environment. One begins with the statement \\begin{array} and ends with the statement \\end{array}. Following the opening statement, a format code is used to indicate the formatting of each column. In the example below, we use the code {rrr} to indicate that each column is right justified. Each row is then entered, with cells separated by the &amp; symbol, and each line (except the last) terminated by \\\\. $$\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} $$ \\[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} \\] In math textbooks, matrices are often surrounded by brackets, and are assigned to a boldface letter. Here is an example $$\\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] $$ \\[\\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] \\] "],
["cheatsheet.html", "6 Cheatsheet 6.1 Dictionary of Symbols 6.2 Including Plots", " 6 Cheatsheet 6.1 Dictionary of Symbols italics bold background highlight Let me write some formlas, e.g. \\(x^2\\) or \\[x^2\\] and that’s it. The *raw* text **is** like **_this_**. $\\frac{x^2}{n-1}$ The raw text is like this. \\(\\frac{x^2}{n-1}\\) Text Effect # First-level Header # First-level Header ## Second-level Header ## Second-level Header ### Third-level Header ### Third-level Header #### Fourth-level Header #### Fourth-level Header ##### Fifth-level Header ##### Fifth-level Header ###### Sixth-level Header ###### Sixth-level Header ####### Seventh-level Header Just regular text! _italic_ italic *italic* italic __bold__ bold **bold** bold X~i~ Xi $X_i$ \\(X_i\\) X^2^ X2 X~i~^2^ Xi2 X^2^~i~ X2i [RStudio](https://www.rstudio.com) RStudio &lt;span style=&quot;font-variant:small-caps;&quot;&gt;Small Caps&lt;/span&gt; Small Caps ![Image Name](path/to/image) ^[This is a footnote] 1 If you do not want a certain heading to be numbered, add {-} after the heading. # Non-numbered Header {-} Unordered list starts with any one of -, +, or *. They are all the same; just pick whichever one you like. You can nest one list within another list by indenting the sub-list with either (a) four spaces or (b) hitting the tab key to create the equivalent of four spaces (e.g., by default, one tab yields two spaces and hence you will need to hit the tab key twice). Unordered list works with only three levels, after which it will just repeat using the symbol intended for the third level. item1 item2 item2.1 item2.2 item2.2.1 item2.2.2 item2.2.2.1 item2.2.2.2 item2.2.2.2.1 item2.2.2.2.2 item1 item2 item2.1 item2.2 item2.2.1 item2.2.2 item2.2.2.1 item2.2.2.2 item2.2.2.2.1 item2.2.2.2.2 item1 item2 item2.1 item2.2 item2.2.1 item2.2.2 item2.2.2.1 item2.2.2.2 item2.2.2.2.1 item2.2.2.2.2 Check out options for knitr: https://yihui.name/knitr/options/ Writing math formulas in markdown: http://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/ 6.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. \\[ab, a b, a b, a~b, a~~b, a~~~b\\] how to get #$%^&amp;, \\cbrt, \\qdrt, \\rddots, x\\hat, x\\tilde 6.2.1 Subscripts and Superscripts \\[ x_i, x_{i+1}, x^2, x^{2/3}, x_i^2, x^2_i, x_{i+1}^{2/3}, \\sqrt{x}, \\sqrt[^n]{x} \\] \\[ \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\] 6.2.2 Operators 1 \\[ \\pm, \\mp, \\times, \\div, =, \\ne, \\le, \\ge, \\ll, \\gg, \\approx, \\sim, \\equiv, \\cong, \\propto, \\infty \\] 6.2.3 Operators 2 \\[ \\exp(x) \\] \\[ \\text{reglar text}, \\boldsymbol{s}, \\mathbf{s} \\] \\[ dx, \\lim, \\partial{x}, \\int, \\sum, \\prod, \\] \\[ \\forall, \\exists, \\nexists, \\cup, \\cap, \\in, \\ni, \\emptyset \\] \\[ \\therefore, \\cdot, \\cdots, \\dots, \\ldots, \\vdots, \\ddots, \\bullet, \\dot A, \\dot \\forall \\] \\[ \\to, \\rightarrow, \\gets, \\leftarrow, \\uparrow, \\downarrow, \\leftrightarrow \\] \\[ \\overset{iid}{\\sim}, \\underset{(1)}{=}, \\underset{\\text{below}}{\\overset{\\text{above}}{=}} \\] \\[ \\nabla \\] This is a footnote↩ "]
]
